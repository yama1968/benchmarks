{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing algorithms on the Internet dataset\n",
    "The data is taken from [this source](https://kdd.ics.uci.edu/databases/internet_usage/internet_usage.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details on running the comparison, see [this article](https://github.com/catboost/benchmarks/blob/master/comparison_description.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/home/yannick/bin/anaconda3/envs/py27/lib/python27.zip',\n",
       " '/home/yannick/bin/anaconda3/envs/py27/lib/python2.7',\n",
       " '/home/yannick/bin/anaconda3/envs/py27/lib/python2.7/plat-linux2',\n",
       " '/home/yannick/bin/anaconda3/envs/py27/lib/python2.7/lib-tk',\n",
       " '/home/yannick/bin/anaconda3/envs/py27/lib/python2.7/lib-old',\n",
       " '/home/yannick/bin/anaconda3/envs/py27/lib/python2.7/lib-dynload',\n",
       " '/home/yannick/bin/anaconda3/envs/py27/lib/python2.7/site-packages',\n",
       " '/home/yannick/bin/anaconda3/envs/py27/lib/python2.7/site-packages/Keras-1.0.5-py2.7.egg',\n",
       " '/home/yannick/bin/anaconda3/envs/py27/lib/python2.7/site-packages/mxnet-0.7.0-py2.7.egg',\n",
       " '/home/yannick/bin/anaconda3/envs/py27/lib/python2.7/site-packages/h2ohyperopt-0.3-py2.7.egg',\n",
       " '/home/yannick/bin/anaconda3/envs/py27/lib/python2.7/site-packages/pymongo-2.2-py2.7-linux-x86_64.egg',\n",
       " '/home/yannick/bin/anaconda3/envs/py27/lib/python2.7/site-packages/pydot_ng-1.0.1.dev0-py2.7.egg',\n",
       " '/home/yannick/bin/anaconda3/envs/py27/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/home/yannick/.ipython',\n",
       " '..']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannick/bin/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from experiment import Experiment\n",
    "from xgboost_experiment import XGBExperiment\n",
    "from lightgbm_experiment import LGBExperiment\n",
    "from catboost_experiment import CABExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and set parameters for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_task = 'classification'\n",
    "dataset_path = '../prepare_internet/'\n",
    "n_estimators = 5000\n",
    "max_hyperopt_evals = 50\n",
    "\n",
    "experiment = Experiment(learning_task, train_path=dataset_path + 'train',\n",
    "                        test_path=dataset_path + 'test', cd_path = dataset_path + 'train.cd')\n",
    "X_train, y_train, X_test, y_test, cat_cols = experiment.read_data()\n",
    "\n",
    "# X_train, y_train = X_train[:1000,:], y_train[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that does the following:\n",
    "\n",
    "**Preprocess the dataset:**\n",
    "calculate counter values on the training set, transform cat features (for the `XGBoost` and `LightGBM` algorithms), and convert the data to the format of the algorithm. Do the same thing with all pairs after splitting them into cross validation folds.\n",
    "\n",
    "**Select the optimal number of trees for the algorithm with the default parameters:** for each fold and each number of trees, get the result of the algorithm trained on the other four folds, average the results for each number of trees, and choose the best one. \n",
    "\n",
    "**Assess the quality of the algorithm with the default parameters on a test dataset:** train the algorithm with the number of trees obtained in the previous step, and calculate the metric value on the test dataset.\n",
    "\n",
    "**Tune parameters for the algorithm using `Hyperopt`:** on each `Hyperopt` iteration of the algorithm, the best value of trees is selected and the metric for cross validation is calculated.\n",
    "\n",
    "**Show results on the test dataset for the algorithm with the tuned parameters:** train the algorithm with the optimal parameters and the number of trees obtained in the previous step, and calculate the metric value on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def run_experiment(Experiment, title):\n",
    "    experiment = Experiment(learning_task, max_hyperopt_evals=max_hyperopt_evals,\n",
    "                            n_estimators=n_estimators)\n",
    "    cv_pairs, (dtrain, dtest) = experiment.split_and_preprocess(X_train.copy(), y_train, \n",
    "                                                                X_test.copy(), y_test, \n",
    "                                                                cat_cols, n_splits=5)\n",
    " \n",
    "    default_cv_result = experiment.run_cv(cv_pairs)\n",
    "    experiment.print_result(default_cv_result, 'Default {} result on cv'.format(title))\n",
    "\n",
    "    print('\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n')\n",
    "\n",
    "    default_test_losses = []\n",
    "    for seed in range(5):\n",
    "        test_result = experiment.run_test(dtrain, dtest, X_test, params=default_cv_result['params'],\n",
    "                                          n_estimators=default_cv_result['best_n_estimators'], seed=seed)\n",
    "        default_test_losses.append(test_result['loss'])\n",
    "        print 'For seed=%d Test\\'s %s : %.5f' % (seed, experiment.metric, default_test_losses[-1])\n",
    "    print '\\nTest\\'s %s mean: %.5f, Test\\'s %s std: %.5f' % (experiment.metric, np.mean(default_test_losses),\n",
    "                                                             experiment.metric, np.std(default_test_losses))\n",
    "\n",
    "    print('\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n')\n",
    "    print('Hyperopt iterations:\\n\\n')\n",
    "\n",
    "    tuned_cv_result = experiment.optimize_params(cv_pairs)\n",
    "\n",
    "    print('\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n')\n",
    "\n",
    "    experiment.print_result(tuned_cv_result, 'Tuned {} result on cv'.format(title))\n",
    "\n",
    "    print('\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n')\n",
    "\n",
    "    tuned_test_losses = []\n",
    "    for seed in range(5):\n",
    "        test_result = experiment.run_test(dtrain, dtest, X_test, params=tuned_cv_result['params'],\n",
    "                                          n_estimators=tuned_cv_result['best_n_estimators'], seed=seed)\n",
    "        tuned_test_losses.append(test_result['loss'])\n",
    "        print 'For seed=%d Test\\'s %s : %.5f' % (seed, experiment.metric, tuned_test_losses[-1])\n",
    "    print '\\nTest\\'s %s mean: %.5f, Test\\'s %s std: %.5f' % (experiment.metric, np.mean(tuned_test_losses),\n",
    "                                                             experiment.metric, np.std(tuned_test_losses))\n",
    "\n",
    "    return np.mean(default_test_losses), np.mean(tuned_test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `XGBoost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb_default_test_result, xgb_tuned_test_result = run_experiment(XGBExperiment, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LightGBM`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "%%time\n",
    "\n",
    "lgb_default_test_result, lgb_tuned_test_result = run_experiment(LGBExperiment, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CatBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import catboost\n",
    "\n",
    "catboost.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default CatBoost result on cv:\n",
      "\n",
      "logloss = 0.22561958074\n",
      "best_n_estimators = 1\n",
      "params = {'rsm': 1.0, 'random_seed': 0, 'verbose': False, 'thread_count': 8, 'learning_rate': 0.03, 'od_type': 'Iter', 'ctr_border_count': 16, 'ctr_description': ['Borders', 'Counter'], 'iterations': 5000, 'od_wait': 40, 'gradient_iterations': 10, 'depth': 6, 'loss_function': 'Logloss', 'fold_len_multiplier': 2, 'kwargs': {}, 'leaf_estimation_method': 'Newton', 'l2_leaf_reg': 3, 'border_count': 128, 'used_ram_limit': 100000000000}\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "For seed=0 Test's logloss : 0.66447\n",
      "For seed=1 Test's logloss : 0.65603\n",
      "For seed=2 Test's logloss : 0.66084\n",
      "For seed=3 Test's logloss : 0.65967\n",
      "For seed=4 Test's logloss : 0.65520\n",
      "\n",
      "Test's logloss mean: 0.65924, Test's logloss std: 0.00337\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Hyperopt iterations:\n",
      "\n",
      "\n",
      "[1/50]\teval_time=29.98 sec\tcurrent_logloss=0.220909\tmin_logloss=0.220909\n",
      "[2/50]\teval_time=140.28 sec\tcurrent_logloss=0.217806\tmin_logloss=0.217806\n",
      "[3/50]\teval_time=68.85 sec\tcurrent_logloss=0.218038\tmin_logloss=0.217806\n",
      "[4/50]\teval_time=15.34 sec\tcurrent_logloss=0.244631\tmin_logloss=0.217806\n",
      "[5/50]\teval_time=9.22 sec\tcurrent_logloss=0.227105\tmin_logloss=0.217806\n",
      "[6/50]\teval_time=415.57 sec\tcurrent_logloss=0.221346\tmin_logloss=0.217806\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cab_default_test_result, cab_tuned_test_result = run_experiment(CABExperiment, \"CatBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Comparing results\n",
    "\n",
    "The final table with metric values on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "%pylab inline --no-import-all\n",
    "\n",
    "test_results = np.array([\n",
    "    (xgb_default_test_result, xgb_tuned_test_result),\n",
    "    (lgb_default_test_result, lgb_tuned_test_result),\n",
    "    (cab_default_test_result, cab_tuned_test_result)\n",
    "])\n",
    "\n",
    "diff = 100 * test_results / test_results[2,1] - 100\n",
    "\n",
    "res = [['{:.6f} ({:+.2f}%)'.format(test_results[i, j], diff[i, j]) for j in range(2)] for i in range(3)]\n",
    "\n",
    "display(HTML(pd.DataFrame(res, columns=['default', 'tuned'], index=['xgboost', 'lightgbm', 'catboost']).to_html()))\n",
    "\n",
    "results = [\n",
    "    ('Tuned CatBoost',   cab_tuned_test_result),\n",
    "    ('Default CatBoost', cab_default_test_result),\n",
    "    ('Tuned XGBoost',    xgb_tuned_test_result),\n",
    "    ('Default XGBoost',  xgb_default_test_result),\n",
    "    ('Tuned LightGBM',   lgb_tuned_test_result),\n",
    "    ('Default LightGBM', lgb_default_test_result),\n",
    "]\n",
    "names = ['%s\\n%.5f' % (name, loss) for name, loss in results]\n",
    "\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.scatter(range(6), zip(*results)[1], s=150)\n",
    "plt.xticks(range(6), names, fontsize=15)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('Comparison', fontsize=20)\n",
    "plt.ylabel(experiment.metric, fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
